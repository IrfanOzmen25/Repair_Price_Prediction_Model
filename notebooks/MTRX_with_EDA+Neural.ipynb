{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Estimating Vehicle Repair Costs with Predictive Modeling\n",
    "\n",
    "Code authored by: Irfan Ozmen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32757,
     "status": "ok",
     "timestamp": 1746029708996,
     "user": {
      "displayName": "Irfan Ozmen",
      "userId": "05914533022160182366"
     },
     "user_tz": 300
    },
    "id": "MpFITUriTNTJ",
    "outputId": "00a5ce6b-cd55-4ca1-c487-e5034e750428"
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%pip install seaborn\n",
    "import seaborn as sns\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "%pip install missingno\n",
    "import missingno as msno\n",
    "%pip install rapidfuzz\n",
    "from rapidfuzz import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nlW_zbZmTfRf"
   },
   "outputs": [],
   "source": [
    "file_path =\"J:/My Drive/Colab Notebooks/RI_Project/MTRX Test Data/MTRX1000000.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 84,
     "status": "ok",
     "timestamp": 1746029709195,
     "user": {
      "displayName": "Irfan Ozmen",
      "userId": "05914533022160182366"
     },
     "user_tz": 300
    },
    "id": "0VDP3ctQ2dy9",
    "outputId": "c6e4cea6-17b2-4889-87f1-8ca339c72ca3"
   },
   "outputs": [],
   "source": [
    "df.info()\n",
    "\n",
    "# List numeric and categorical features\n",
    "numeric_cols = df.select_dtypes(include='number').columns\n",
    "categorical_cols = df.select_dtypes(include='object').columns\n",
    "\n",
    "print(\"Numerical Features:\\n\", numeric_cols)\n",
    "print(\"\\nCategorical Features:\\n\", categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 918
    },
    "executionInfo": {
     "elapsed": 1812,
     "status": "ok",
     "timestamp": 1746029711008,
     "user": {
      "displayName": "Irfan Ozmen",
      "userId": "05914533022160182366"
     },
     "user_tz": 300
    },
    "id": "ZdphlyT24dYf",
    "outputId": "9da109da-d176-41f6-ab28-66f9a03a3b04"
   },
   "outputs": [],
   "source": [
    "#Histograms for numeric columns\n",
    "df[numeric_cols].hist(bins=30, figsize=(15, 10))\n",
    "plt.suptitle(\"Numerical Feature Distributions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 988,
     "status": "ok",
     "timestamp": 1746029711998,
     "user": {
      "displayName": "Irfan Ozmen",
      "userId": "05914533022160182366"
     },
     "user_tz": 300
    },
    "id": "PzV3qo3W4gNT",
    "outputId": "5b9cd65d-6552-486b-b243-91deff46690f"
   },
   "outputs": [],
   "source": [
    "#Counterplots for top categorical features\n",
    "for col in ['ITEM_NAME', 'VEHICLE_MODEL', 'REPAIRER_STATE']:\n",
    "    plt.figure(figsize=(10,4))\n",
    "    sns.countplot(y=col, data=df, order=df[col].value_counts().index[:10])\n",
    "    plt.title(f\"Top values in {col}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 946
    },
    "executionInfo": {
     "elapsed": 1157,
     "status": "ok",
     "timestamp": 1746029714049,
     "user": {
      "displayName": "Irfan Ozmen",
      "userId": "05914533022160182366"
     },
     "user_tz": 300
    },
    "id": "i8XGPNsZ6PCJ",
    "outputId": "28b31f06-6ea2-4e83-9c77-b3cc8aa7d96e"
   },
   "outputs": [],
   "source": [
    "for col in numeric_cols:\n",
    "    plt.figure(figsize=(10,2))\n",
    "    sns.boxplot(x=df[col])\n",
    "    plt.title(f\"Outlier Detection in {col}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 280,
     "status": "ok",
     "timestamp": 1746029714328,
     "user": {
      "displayName": "Irfan Ozmen",
      "userId": "05914533022160182366"
     },
     "user_tz": 300
    },
    "id": "lS_sRCZU6ahw",
    "outputId": "ecc769ba-bfdd-4c5e-de54-d66dc7c1cb3d"
   },
   "outputs": [],
   "source": [
    "#Handle Missing values\n",
    "\n",
    "#Item mapping\n",
    "\n",
    "# List of accepted categories\n",
    "clean_categories = [\n",
    "    \"HAIL\",\n",
    "    \"PDR\",\n",
    "    \"GLASS\",\n",
    "    \"PACKAGE\",\n",
    "    \"PPF VINYL PACKAGE\",\n",
    "    \"PARTS\",\n",
    "    \"WHEEL\",\n",
    "    \"TINT\",\n",
    "    \"MISC\",\n",
    "    \"DETAIL\",\n",
    "    \"PPF VINYL\",\n",
    "    \"REMOVEINSTALL\",\n",
    "    \"PRICE A DENT\",\n",
    "    \"PAINT & BODY\",\n",
    "    \"DETAIL PACKAGE\",\n",
    "    \"PPF VINYL MATERIAL\"\n",
    "]\n",
    "\n",
    "def fuzzy_clean_item_name(raw_name):\n",
    "    try:\n",
    "        name = str(raw_name).upper().strip()\n",
    "        best_match, score, _ = process.extractOne(name, clean_categories)\n",
    "        return best_match if score > 80 else name\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping value: {raw_name} due to error: {e}\")\n",
    "        return raw_name  # fallback if something goes wrong\n",
    "\n",
    "df['SERVICE'] = df['SERVICE'].apply(fuzzy_clean_item_name).str.upper().str.strip()\n",
    "\n",
    "\n",
    "\n",
    "# Make a copy to preserve original\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "# Strategy examples:\n",
    "for col in numeric_cols:\n",
    "    df_cleaned[col].fillna(df_cleaned[col].mean(), inplace=True)\n",
    "\n",
    "for col in categorical_cols:\n",
    "    df_cleaned[col].fillna(df_cleaned[col].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1746029714336,
     "user": {
      "displayName": "Irfan Ozmen",
      "userId": "05914533022160182366"
     },
     "user_tz": 300
    },
    "id": "8tHiTkF03gLv",
    "outputId": "f267bb65-bcba-454e-fd69-61bac0afa258"
   },
   "outputs": [],
   "source": [
    "print(\"Numerical Features:\\n\", numeric_cols)\n",
    "print(\"\\nCategorical Features:\\n\", categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1746029714339,
     "user": {
      "displayName": "Irfan Ozmen",
      "userId": "05914533022160182366"
     },
     "user_tz": 300
    },
    "id": "CYF2xlRD6sc0",
    "outputId": "985734eb-89d8-4da8-ddd0-991bfc8c6117"
   },
   "outputs": [],
   "source": [
    "# Test Outliers with IQR method\n",
    "Q1 = df_cleaned[numeric_cols].quantile(0.25)\n",
    "Q3 = df_cleaned[numeric_cols].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "df_no_outliers = df_cleaned[~((df_cleaned[numeric_cols] < (Q1 - 1.5 * IQR)) |\n",
    "                              (df_cleaned[numeric_cols] > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "print(\"Original size:\", df_cleaned.shape)\n",
    "print(\"No outlier size:\", df_no_outliers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "df_model = df_no_outliers.copy()  # or df_cleaned if you choose not to remove outliers\n",
    "\n",
    "TARGET = \"ITEM_AMOUNT\"\n",
    "\n",
    "# choose which categorical columns you want embeddings for\n",
    "cat_cols = [\"ITEM_NAME\", \"VEHICLE_MODEL\", \"REPAIRER_STATE\", \"SERVICE\"]  # adjust as needed\n",
    "\n",
    "# numeric columns = numeric features except target\n",
    "num_cols = [c for c in df_model.select_dtypes(include=\"number\").columns if c != TARGET]\n",
    "# Remove identifier columns (edit list based on dataset)\n",
    "id_like = [\"INVOICE_ID\", \"_BATCH_ID_\"]\n",
    "num_cols = [c for c in num_cols if c not in id_like]\n",
    "\n",
    "# 1) split FIRST (prevents leakage)\n",
    "df_model = df_model[df_model[TARGET].notna()].copy()\n",
    "df_model = df_model[df_model[TARGET] >= 0].copy()\n",
    "\n",
    "train_df, test_df = train_test_split(df_model, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2) winsorize using TRAIN percentiles only\n",
    "low_q, high_q = train_df[TARGET].quantile([0.005, 0.995])\n",
    "train_df[TARGET] = train_df[TARGET].clip(lower=low_q, upper=high_q)\n",
    "test_df[TARGET]  = test_df[TARGET].clip(lower=low_q, upper=high_q)\n",
    "\n",
    "# 3) scale numeric (fit on train only)\n",
    "scaler = StandardScaler()\n",
    "X_num_train = scaler.fit_transform(train_df[num_cols].astype(float))\n",
    "X_num_test  = scaler.transform(test_df[num_cols].astype(float))\n",
    "\n",
    "y_train = np.log1p(train_df[TARGET].astype(np.float32).values).reshape(-1, 1)\n",
    "y_test  = np.log1p(test_df[TARGET].astype(np.float32).values).reshape(-1, 1)\n",
    "\n",
    "# 4) build vocab (train only) and encode cats. Adding a range will work better. Second options is much more diffucult.\n",
    "def build_vocab(series: pd.Series, min_freq=2):\n",
    "    vc = series.astype(str).value_counts()\n",
    "    tokens = vc[vc >= min_freq].index.tolist()\n",
    "    # 0 reserved for UNK\n",
    "    stoi = {tok: i+1 for i, tok in enumerate(tokens)}\n",
    "    return stoi\n",
    "\n",
    "vocabs = {col: build_vocab(train_df[col], min_freq=2) for col in cat_cols}\n",
    "\n",
    "def encode_cats(frame: pd.DataFrame, vocabs: dict, cat_cols: list):\n",
    "    cols = []\n",
    "    for col in cat_cols:\n",
    "        stoi = vocabs[col]\n",
    "        ids = frame[col].astype(str).map(lambda x: stoi.get(x, 0)).astype(np.int64).values\n",
    "        cols.append(ids)\n",
    "    return np.stack(cols, axis=1)  # shape [N, num_cat_cols]\n",
    "\n",
    "X_cat_train = encode_cats(train_df, vocabs, cat_cols)\n",
    "X_cat_test  = encode_cats(test_df, vocabs, cat_cols)\n",
    "\n",
    "# 5) embedding sizes\n",
    "def emb_dim(cardinality: int) -> int:\n",
    "    return int(min(50, round(np.sqrt(cardinality))))\n",
    "\n",
    "cat_cardinalities = {col: len(vocabs[col]) + 1 for col in cat_cols}  # +1 for UNK=0\n",
    "emb_dims = {col: emb_dim(cat_cardinalities[col]) for col in cat_cols}\n",
    "\n",
    "cat_cardinalities, emb_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class RepairDataset(Dataset):\n",
    "    def __init__(self, X_cat, X_num, y):\n",
    "        self.X_cat = torch.tensor(X_cat, dtype=torch.long)\n",
    "        self.X_num = torch.tensor(X_num, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_cat[idx], self.X_num[idx], self.y[idx]\n",
    "\n",
    "train_ds = RepairDataset(X_cat_train, X_num_train, y_train)\n",
    "test_ds  = RepairDataset(X_cat_test,  X_num_test,  y_test)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=256, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=1024, shuffle=False)\n",
    "\n",
    "\n",
    "class RepairEmbNet(nn.Module):\n",
    "    def __init__(self, cat_cardinalities, emb_dims, n_num, hidden=128, p=0.2):\n",
    "        super().__init__()\n",
    "        self.cat_cols = list(cat_cardinalities.keys())\n",
    "\n",
    "        self.emb = nn.ModuleDict({\n",
    "            col: nn.Embedding(cat_cardinalities[col], emb_dims[col])\n",
    "            for col in self.cat_cols\n",
    "        })\n",
    "\n",
    "        emb_total = sum(emb_dims.values())\n",
    "        self.bn_num = nn.BatchNorm1d(n_num) if n_num > 0 else None\n",
    "\n",
    "        self.fc1 = nn.Linear(emb_total + n_num, hidden)\n",
    "        self.drop1 = nn.Dropout(p)\n",
    "        self.fc2 = nn.Linear(hidden, hidden // 2)\n",
    "        self.drop2 = nn.Dropout(p)\n",
    "        self.out = nn.Linear(hidden // 2, 1)\n",
    "\n",
    "    def forward(self, x_cat, x_num):\n",
    "        embs = [self.emb[col](x_cat[:, i]) for i, col in enumerate(self.cat_cols)]\n",
    "        x = torch.cat(embs, dim=1)\n",
    "\n",
    "        if self.bn_num is not None:\n",
    "            x_num = self.bn_num(x_num)\n",
    "        x = torch.cat([x, x_num], dim=1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.drop2(x)\n",
    "        return self.out(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = RepairEmbNet(cat_cardinalities, emb_dims, n_num=len(num_cols)).to(device)\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "# Huber loss (SmoothL1). beta is the \"delta\" threshold in log-space.\n",
    "loss_fn = nn.SmoothL1Loss(beta=0.5) # try 0.1, 0.2, 0.5 \n",
    "\n",
    "numepochs = 20\n",
    "train_loss_hist = []\n",
    "test_loss_hist  = []\n",
    "\n",
    "for epoch in range(numepochs):\n",
    "    # ---- train ----\n",
    "    model.train()\n",
    "    batch_losses = []\n",
    "\n",
    "    for xc, xn, yb in train_loader:\n",
    "        xc, xn, yb = xc.to(device), xn.to(device), yb.to(device)\n",
    "\n",
    "        pred = model(xc, xn)\n",
    "        loss = loss_fn(pred, yb)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "\n",
    "        batch_losses.append(loss.item())\n",
    "\n",
    "    train_epoch_loss = float(np.mean(batch_losses))\n",
    "    train_loss_hist.append(train_epoch_loss)\n",
    "\n",
    "    # ---- eval (loss + metrics) ----\n",
    "    model.eval()\n",
    "    eval_losses = []\n",
    "    preds, trues = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xc, xn, yb in test_loader:\n",
    "            xc, xn, yb = xc.to(device), xn.to(device), yb.to(device)\n",
    "\n",
    "            pred = model(xc, xn)\n",
    "            loss = loss_fn(pred, yb)\n",
    "            eval_losses.append(loss.item())\n",
    "\n",
    "            preds.append(pred.cpu().numpy())\n",
    "            trues.append(yb.cpu().numpy())\n",
    "\n",
    "    test_epoch_loss = float(np.mean(eval_losses))\n",
    "    test_loss_hist.append(test_epoch_loss)\n",
    "\n",
    "    # stack first (critical)\n",
    "    preds = np.vstack(preds)   # shape [N,1]\n",
    "    trues = np.vstack(trues)   # shape [N,1]\n",
    "    \n",
    "\n",
    "    # convert log-space -> dollar-space\n",
    "    preds_cost = np.expm1(preds)\n",
    "    trues_cost = np.expm1(trues)\n",
    "\n",
    "    mse = mean_squared_error(trues_cost, preds_cost)\n",
    "    mae = mean_absolute_error(trues_cost, preds_cost)\n",
    "    r2  = r2_score(trues_cost, preds_cost)\n",
    "\n",
    "    # Number of samples in evaluation\n",
    "    n = len(trues_cost)\n",
    "\n",
    "    # Effective number of predictors\n",
    "    p = len(num_cols) + sum(emb_dims.values())\n",
    "\n",
    "    # Adjusted R^2 (guard against invalid case)\n",
    "    if n > p + 1:\n",
    "        adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "    else:\n",
    "        adj_r2 = np.nan\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1:02d} | \"\n",
    "        f\"train huber {train_epoch_loss:.4f} | \"\n",
    "        f\"test huber {test_epoch_loss:.4f} | \"\n",
    "        f\"MAE ${mae:.2f} | R2 {r2:.3f} | \"\n",
    "    )\n",
    "# preds and trues are already stacked and in log-space\n",
    "log_residuals = (trues - preds).reshape(-1)\n",
    "\n",
    "# Remove any non-finite values (defensive)\n",
    "log_residuals = log_residuals[np.isfinite(log_residuals)]\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "stats.probplot(log_residuals, dist=\"norm\", plot=plt)\n",
    "plt.title(\"Q–Q Plot of Log-Space Residuals\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = np.arange(1, len(train_loss_hist) + 1)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(epochs, train_loss_hist, label=\"Train Loss (MSE)\")\n",
    "plt.plot(epochs, test_loss_hist, label=\"Test Loss (MSE)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training vs Test Loss Over Epochs\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure 1D arrays\n",
    "y_pred = preds.reshape(-1)\n",
    "y_true = trues.reshape(-1)\n",
    "\n",
    "y_pred_cost = np.expm1(y_pred)\n",
    "y_true_cost = np.expm1(y_true)\n",
    "\n",
    "residuals = y_true_cost - y_pred_cost\n",
    "\n",
    "#################################################################################\n",
    "# Attach predictions & residuals back to test_df (align by index)\n",
    "test_df_diag = test_df.copy().reset_index(drop=True)\n",
    "\n",
    "test_df_diag[\"y_pred_cost\"] = y_pred_cost\n",
    "test_df_diag[\"residual\"] = residuals\n",
    "\n",
    "# Identify extreme suspicious points\n",
    "bad = test_df_diag.loc[\n",
    "    (test_df_diag[\"y_pred_cost\"] > 800) &\n",
    "    (test_df_diag[\"residual\"] < -800)\n",
    "]\n",
    "\n",
    "bad[[TARGET, \"VEHICLE_MODEL\", \"SERVICE\", \"REPAIRER_STATE\", \"y_pred_cost\", \"residual\"]]\n",
    "#################################################################################################\n",
    "\n",
    "# ---- Empirical prediction interval from residuals (dollars) ----\n",
    "# Choose coverage (80% interval shown here: 10th to 90th percentile)\n",
    "lo_q, hi_q = 0.10, 0.90\n",
    "\n",
    "res_lo = np.quantile(residuals, lo_q)\n",
    "res_hi = np.quantile(residuals, hi_q)\n",
    "\n",
    "print(f\"Empirical residual interval ({int((hi_q-lo_q)*100)}%): \"\n",
    "      f\"[{res_lo:.2f}, {res_hi:.2f}] dollars\")\n",
    "\n",
    "print(\"Residual summary:\")\n",
    "print(f\"Mean residual: {residuals.mean():.2f}\")\n",
    "print(f\"Std residual:  {residuals.std():.2f}\")\n",
    "print(f\"MAE:           {np.mean(np.abs(residuals)):.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "# For visualization only\n",
    "plot_residuals = residuals.copy()\n",
    "plot_residuals = np.clip(plot_residuals, -400, 400)\n",
    "\n",
    "# 1) Residuals vs Predicted\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(y_pred_cost, residuals, alpha=0.4)\n",
    "plt.axhline(0, linewidth=2)\n",
    "plt.xlabel(\"Predicted Cost ($)\")\n",
    "plt.ylabel(\"Residual (Actual - Predicted)\")\n",
    "plt.title(\"Residuals vs Predicted Cost\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 2) Residual Histogram\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(residuals, bins=40, edgecolor=\"black\", alpha=0.8)\n",
    "plt.axvline(0, linewidth=2)\n",
    "plt.xlabel(\"Residual (Actual - Predicted)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Residual Histogram\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = pd.DataFrame([{\n",
    "    \"VEHICLE_YEAR\": 2017,\n",
    "    \"ITEM_NAME\": \"FRONT\",\n",
    "    \"VEHICLE_MODEL\": \"F150\",\n",
    "    \"REPAIRER_STATE\": \"California\",\n",
    "    \"SERVICE\": \"PDR\"\n",
    "}])\n",
    "\n",
    "# numeric: align columns and fill missing with train means\n",
    "num_fill = train_df[num_cols].astype(float).mean()\n",
    "example_num = example.reindex(columns=num_cols, fill_value=np.nan).astype(float)\n",
    "example_num = example_num.fillna(num_fill)\n",
    "\n",
    "ex_num = scaler.transform(example_num)\n",
    "# categorical\n",
    "ex_cat = encode_cats(example, vocabs, cat_cols)\n",
    "\n",
    "xc = torch.tensor(ex_cat, dtype=torch.long).to(device)\n",
    "xn = torch.tensor(ex_num, dtype=torch.float32).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred_log = model(xc, xn).item()\n",
    "pred_cost = np.expm1(pred_log)\n",
    "\n",
    "# ---- Prediction range using empirical residual interval ---\n",
    "\n",
    "pred_low  = max(0.0, pred_cost + res_lo)\n",
    "pred_high = max(0.0, pred_cost + res_hi)\n",
    "\n",
    "print(f\"Predicted repair cost (point): ${pred_cost:.2f}\")\n",
    "print(f\"Likely range ({int((hi_q-lo_q)*100)}%): ${pred_low:.2f} – ${pred_high:.2f}\")\n",
    "print(f\"Uncertainty (half-width): ±${(pred_high - pred_low)/2:.2f}\")\n",
    "\n",
    "print(f\"Predicted repair cost: ${pred_cost:.2f}\")\n",
    "\n",
    "\n",
    "baseline_log = np.full_like(y_test, y_train.mean())\n",
    "baseline_pred = np.expm1(baseline_log)\n",
    "baseline_true = np.expm1(y_test)\n",
    "\n",
    "baseline_mae = mean_absolute_error(baseline_true, baseline_pred)\n",
    "\n",
    "print(f\"Baseline MAE (dollars): {baseline_mae:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
